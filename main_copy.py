import os
import sounddevice as sd
import soundfile as sf
import numpy as np
import sys, types, importlib.util

# ‚úÖ patch ‡∏Å‡πà‡∏≠‡∏ô joblib
if importlib.util.find_spec("numpy.random._mt19937") is None:
    fake_module = types.ModuleType("numpy.random._mt19937")
    fake_module.MT19937 = np.random.MT19937
    sys.modules["numpy.random._mt19937"] = fake_module

import threading
import librosa
import yaml
import matplotlib.pyplot as plt
import librosa.display
import torch
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from joblib import load




# ------------------------- CONFIG -------------------------
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

samplerate = config["samplerate"]
channels = config["channels"]
duration_limit = config["duration_limit"]

recorded_frames = []

speaker_no = config["speaker_no"]
correction = config["correction"]
filename = config["filename"]

output_folder = os.path.join("dataset_eng", speaker_no, correction)
os.makedirs(output_folder, exist_ok=True)
filepath = os.path.join(output_folder, filename)

# ------------------------- RECORDING -------------------------
def audio_callback(indata, frames, time, status):
    volume_norm = np.linalg.norm(indata) * 10
    print(
        "‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏°‡∏Ñ‡πå:",
        "üü¢" if volume_norm > 0.1 else "üî¥",
        f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á: {volume_norm:.2f}",
        end="\r",
    )
    recorded_frames.append(indata.copy())

def wait_for_enter():
    input("‡∏Å‡∏î Enter ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á...")
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á...")
    stream.stop()

input("‡∏Å‡∏î Enter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πÇ‡∏Ñ‡∏£‡πÇ‡∏ü‡∏ô...")

stream = sd.InputStream(
    callback=audio_callback, channels=channels, samplerate=samplerate
)
print("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß... ‡∏û‡∏π‡∏î‡πÉ‡∏™‡πà‡πÑ‡∏°‡∏Ñ‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö!")
stream.start()

thread = threading.Thread(target=wait_for_enter)
thread.start()
sd.sleep(duration_limit * 1000)

if stream.active:
    print("‡∏´‡∏¢‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏£‡∏ö‡πÄ‡∏ß‡∏•‡∏≤")
    stream.stop()

stream.close()
audio_data = np.concatenate(recorded_frames, axis=0)
sf.write(filepath, audio_data, samplerate)
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô '{filepath}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!")

# ------------------------- PREPROCESSING -------------------------
print("\nüßπ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Preprocessing (‡∏ï‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö + Padding)...")

top_db = 30
y, sr = librosa.load(filepath, sr=samplerate)
y_trimmed, index = librosa.effects.trim(y, top_db=top_db)
pad = int(0.05 * sr)
y_padded = np.pad(y_trimmed, (pad, pad), mode="constant")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á preprocessing
preprocessed_path = filepath.replace(".wav", "_preprocessed.wav")
sf.write(preprocessed_path, y_padded, samplerate)
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á Preprocessing ‡πÄ‡∏õ‡πá‡∏ô '{preprocessed_path}'")

# ------------------------- FEATURE EXTRACTION -------------------------
print("\nüéß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Feature Extraction (Wav2Vec2)...")

MODEL_NAME = "airesearch/wav2vec2-large-xlsr-53-th"

processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
model = Wav2Vec2Model.from_pretrained(MODEL_NAME)
model.eval()

inputs = processor(y_padded, sampling_rate=samplerate, return_tensors="pt", padding=True)

with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.squeeze(0).mean(dim=0).numpy()

embedding_path = preprocessed_path.replace(".wav", "_embedding.npy")
np.save(embedding_path, embeddings)
print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Embedding ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {embedding_path}")

# ------------------------- SCALING -------------------------
print("\nüìè ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Scaling (‡πÉ‡∏ä‡πâ StandardScaler ‡∏à‡∏≤‡∏Å .pkl)...")

# ‡πÇ‡∏´‡∏•‡∏î Scaler ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
scaler_path = "model/embedding_scaler_extend.pkl"  # ‡∏£‡∏∞‡∏ö‡∏∏ path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå .pkl ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
scaler = load(scaler_path)

# ‡πÅ‡∏õ‡∏•‡∏á embedding ‡∏î‡πâ‡∏ß‡∏¢ Scaler ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤
embedding_scaled = scaler.transform([embeddings])

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á Scaling
scaled_path = embedding_path.replace(".npy", "_scaled.npy")
np.save(scaled_path, embedding_scaled)
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á Scaling ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {scaled_path}")

# ------------------------- CLASSIFICATION -------------------------
print("\nüß† ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Classification (‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• MLP ‡∏à‡∏≤‡∏Å .npz)...")

import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelBinarizer

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .npz
mlp_data = np.load("model/mlp_speed_classifier_extend_v2.npz", allow_pickle=True)

coefs = [np.array(w) for w in mlp_data["coefs"]]          # list[np.ndarray]
intercepts = [np.array(b) for b in mlp_data["intercepts"]]# list[np.ndarray]
classes_raw = mlp_data["classes"]                         # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏≤‡∏à‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏•‡∏µ‡πà classes ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 1-D ‡∏Ç‡∏≠‡∏á‡∏™‡πÄ‡∏Å‡∏•‡∏≤‡∏£‡πå (‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ñ‡∏™) ---
def normalize_classes(x):
    c = x
    # ‡∏Ñ‡∏•‡∏µ‡πà tuple/list ‡∏ó‡∏µ‡πà‡∏´‡πà‡∏≠‡∏°‡∏≤‡πÅ‡∏Ñ‡πà‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ã‡πâ‡∏≥ ‡πÜ ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 1-element container
    while isinstance(c, (tuple, list, np.ndarray)) and np.size(c) == 1:
        c = c[0]
    c = np.asarray(c, dtype=object)

    # ‡∏ñ‡πâ‡∏≤ element ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô array/list/tuple ‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏à‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏Å‡∏•‡∏≤‡∏£‡πå
    while c.dtype == object and c.size > 0 and isinstance(c[0], (np.ndarray, list, tuple)):
        c = np.asarray(c[0], dtype=object)

    # ‡πÅ‡∏ö‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 1-D
    c = np.ravel(c)

    # ‡πÅ‡∏õ‡∏•‡∏á numpy scalar ‚Üí python scalar ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô unique_labels ‡∏á‡∏≠‡πÅ‡∏á
    out = []
    for v in c:
        if isinstance(v, np.generic):
            out.append(np.asarray(v).item())
        else:
            out.append(v)
    return np.array(out, dtype=object)

classes = normalize_classes(classes_raw)

# (‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™ edge) ‡∏ñ‡πâ‡∏≤ classes ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏ã‡πâ‡∏≥ ‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏¢‡∏π‡∏ô‡∏µ‡∏Å‡πå
if classes.size == 0:
    raise ValueError("Classes from NPZ is empty.")
classes_unique = np.array(list(dict.fromkeys(list(classes))))  # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° + unique

# ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á hidden layer ‡∏à‡∏≤‡∏Å weights ‡πÄ‡∏î‡∏¥‡∏° (‡πÑ‡∏°‡πà‡πÄ‡∏î‡∏≤)
hidden_layers = [w.shape[1] for w in coefs[:-1]]

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á MLP ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
mlp_model = MLPClassifier(hidden_layer_sizes=tuple(hidden_layers))
mlp_model.coefs_ = coefs
mlp_model.intercepts_ = intercepts
mlp_model.classes_ = classes_unique

# ‚úÖ ‡πÄ‡∏ï‡∏¥‡∏°‡πÅ‡∏≠‡∏ï‡∏ó‡∏£‡∏¥‡∏ö‡∏¥‡∏ß‡∏ï‡πå‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ó‡∏µ‡πà scikit-learn ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ
mlp_model.n_layers_ = len(coefs) + 1
mlp_model.n_outputs_ = len(classes_unique)
mlp_model.out_activation_ = "softmax"

# ‚úÖ LabelBinarizer ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-class ‡πÑ‡∏î‡πâ (‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏ß‡∏ô‡∏™‡∏±‡∏ö‡∏™‡∏ô‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡∏ñ‡∏π‡∏Å‡πÅ‡∏•‡πâ‡∏ß)
lb = LabelBinarizer()
lb.fit(classes_unique)                 # fit ‡∏î‡πâ‡∏ß‡∏¢‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ñ‡∏•‡∏≤‡∏™ (‡∏ó‡∏≥ one-hot ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™)
mlp_model._label_binarizer = lb

# ‚úÖ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
try:
    y_pred = mlp_model.predict(embedding_scaled)
    y_prob = mlp_model.predict_proba(embedding_scaled)
except Exception as e:
    # --- Fallback ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ sklearn ‡∏á‡∏≠‡πÅ‡∏á: ‡∏ó‡∏≥ forward ‡πÄ‡∏≠‡∏á‡πÅ‡∏ö‡∏ö manual ---
    def forward_once(x, Ws, bs):
        a = x
        for W, b in zip(Ws[:-1], bs[:-1]):
            a = np.maximum(0.0, a @ W + b)  # ReLU
        logits = a @ Ws[-1] + bs[-1]
        # softmax ‡πÅ‡∏ö‡∏ö stable
        z = logits - np.max(logits, axis=1, keepdims=True)
        exp = np.exp(z)
        probs = exp / np.sum(exp, axis=1, keepdims=True)
        return probs

    y_prob = forward_once(embedding_scaled.astype(np.float64), coefs, intercepts)
    # map index ‚Üí label ‡∏ï‡∏≤‡∏° classes_unique
    idx = int(np.argmax(y_prob, axis=1)[0])
    y_pred = np.array([classes_unique[idx]], dtype=object)

predicted_label = y_pred[0]
confidence = float(np.max(y_prob[0]) * 100.0)

# ------------------------- OUTPUT -------------------------
print("\nüì¢ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Output (‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å)...")
print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
print(f"üéØ Label ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ: {predicted_label}")
print(f"üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {confidence:.2f}%")
print("\nüìà ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™:")

# ‡∏ß‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ probability ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™ (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° classes ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å)
for cls, prob in zip(classes_unique, y_prob[0]):
    print(f"   - {cls:<10} : {prob * 100:.2f}%")

print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå .txt ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á
result_path = scaled_path.replace(".npy", "_result.txt")
with open(result_path, "w", encoding="utf-8") as f:
    f.write(f"Label: {predicted_label}\n")
    f.write(f"Confidence (Top): {confidence:.2f}%\n")
    f.write("Confidence by Class:\n")
    for cls, prob in zip(classes_unique, y_prob[0]):
        f.write(f"   - {cls:<10} : {prob * 100:.2f}%\n")

print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå: {result_path}")

